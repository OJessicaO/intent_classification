{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import json\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_full.json') as f:\n",
    "    full_data = json.load(f)\n",
    "train, val, test, labels = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose 20 random labels\n",
    "train_index, val_index, test_index = 500, 100, 150\n",
    "for i in range(20):\n",
    "    labels.append(full_data['train'][train_index][1])\n",
    "    train += full_data['train'][train_index : train_index+100]\n",
    "    train_index += 100\n",
    "    val += full_data['val'][val_index : val_index+20]\n",
    "    val_index += 20\n",
    "    test += full_data['test'][test_index : test_index+30]\n",
    "    test_index += 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['insurance_change', 'find_phone', 'travel_alert', 'pto_request', 'improve_credit_score', 'fun_fact', 'change_language', 'payday', 'replacement_card_duration', 'time', 'application_status', 'flight_status', 'flip_coin', 'change_user_name', 'where_are_you_from', 'shopping_list_update', 'what_can_i_ask_you', 'maybe', 'oil_change_how', 'restaurant_reservation']\n"
     ]
    }
   ],
   "source": [
    "# check the chosen labels\n",
    "print(labels)\n",
    "label_dict = {}\n",
    "for i in range(20):\n",
    "    label_dict[labels[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset from the chosen labels\n",
    "train = pd.DataFrame(train, columns = ['text', 'label'])\n",
    "val = pd.DataFrame(train, columns = ['text', 'label'])\n",
    "test = pd.DataFrame(train, columns = ['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantify the labels\n",
    "train['label'] = train['label'].apply(lambda x: label_dict[x])\n",
    "val['label'] = val['label'].apply(lambda x: label_dict[x])\n",
    "test['label'] = test['label'].apply(lambda x: label_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store lengths for the text data\n",
    "train['length'] = train['text'].apply(lambda x: len(x.split()))\n",
    "val['length'] = val['text'].apply(lambda x: len(x.split()))\n",
    "test['length'] = test['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text data\n",
    "tok = spacy.load('en')\n",
    "def tokenize (text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return [token.text for token in tok.tokenizer(nopunct)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary from words and convert to indices\n",
    "counts = Counter()\n",
    "for index, row in train.iterrows():\n",
    "    counts.update(tokenize(row['text']))\n",
    "    \n",
    "for word in list(counts):\n",
    "    if counts[word] < 2:\n",
    "        del counts[word]\n",
    "        \n",
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(text, vocab2index, N=10):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store encoded vectors and their lengths for the sentences\n",
    "train['encoded'] = train['text'].apply(lambda x: np.array(encode_sentence(x,vocab2index ), dtype=object))\n",
    "val['encoded'] = val['text'].apply(lambda x: np.array(encode_sentence(x,vocab2index ), dtype=object))\n",
    "test['encoded'] = test['text'].apply(lambda x: np.array(encode_sentence(x,vocab2index ), dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset in required form\n",
    "X_train, Y_train = train['encoded'], train['label']\n",
    "X_val, Y_val = val['encoded'], val['label']\n",
    "X_test, Y_test = test['encoded'], test['label']\n",
    "\n",
    "train_ds = IntentsDataset(X_train, Y_train)\n",
    "valid_ds = IntentsDataset(X_val, Y_val)\n",
    "test_ds = IntentsDataset(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluation\n",
    "def train_model(model, epochs=10, lr=0.001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y, l in train_dl:\n",
    "            x = x.long()\n",
    "            y = y.long()\n",
    "            y_pred = model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc, val_rmse = validation_metrics(model, val_dl)\n",
    "        if i % 5 == 1:\n",
    "            print(\"train loss %.3f, val loss %.3f, val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))\n",
    "\n",
    "def validation_metrics (model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    for x, y, l in valid_dl:\n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "        y_hat = model(x, l)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
    "    return sum_loss/total, correct/total, sum_rmse/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialization\n",
    "batch_size = 50\n",
    "vocab_size = len(words)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=batch_size)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the lstm model\n",
    "class LSTM(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 20)\n",
    "        \n",
    "    def forward(self, x, s):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        x_pack = pack_padded_sequence(x, s, batch_first=True, enforce_sorted=False)\n",
    "        out_pack, (ht, ct) = self.lstm(x_pack)\n",
    "        out = self.linear(ht[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(vocab_size, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.443, val loss 0.136, val accuracy 0.969\n",
      "train loss 0.051, val loss 0.008, val accuracy 0.999\n",
      "train loss 0.015, val loss 0.002, val accuracy 1.000\n",
      "train loss 0.013, val loss 0.006, val accuracy 0.999\n",
      "train loss 0.014, val loss 0.001, val accuracy 1.000\n",
      "train loss 0.011, val loss 0.001, val accuracy 1.000\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train_model(model, epochs=30, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss 0.000, test accuracy 1.000\n"
     ]
    }
   ],
   "source": [
    "# testing on test dataset\n",
    "test_loss, test_acc, test_rmse = validation_metrics(model, test_dl)\n",
    "print(\"test loss %.3f, test accuracy %.3f\" % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Input feature representation: For the purpose of saving time I decided to go with a simple bag of words model by counting the number of word in the vocabulary and assigning them an index.\n",
    "\n",
    "Model Architecture: I chose to use an LSTM because it can best encode memory into a sentence and retain older information and relationships between parts of sentences that a simple linear classifier cannot.\n",
    "\n",
    "Training parameters: Again, for shortage of time, I did a simple tuning of the learning rate for a good loss decrease but could not exoeriment much with embedding sizes or multiple lstm layers.\n",
    "\n",
    "Evaluation: Cross Entropy Loss works well for classification in balanced dataset, so I decided to go with that. I also check accuracy as is custom.\n",
    "\n",
    "Future work: Improve feature representations using learned embeddings like word2vec or glove. Tune other hyperparameters such as embedding size, hidden size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
